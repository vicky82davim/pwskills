{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20526d54-5408-4ae9-89e4-944fcf40cd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Simple linear regression comprises one independent and one dependent variable, whereas, \\nmultiple linear regression comprises two or more independent variables and one dependent variable.\\n\\n2. It is assumed that for any specific set of values of the independent variable, the regression equation is \\nassociated with a random error, e. The random errors are normally distributed with mean 0 and standard deviation, \\n. Finally, the random errors are independent of each other.\\n\\n3. Find the general formula of the multiple regression as shown below:\\ny^ = b0+b1x1+b2x2+b3x3......bkxk\\n\\nwhere k denotes the number of predictor variables, b0,b1,b2....bk denote the coefficients of the independent \\nvariables x1,x2,x3.....xk\\n.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "1. Simple linear regression comprises one independent and one dependent variable, whereas, \n",
    "multiple linear regression comprises two or more independent variables and one dependent variable.\n",
    "\n",
    "2. It is assumed that for any specific set of values of the independent variable, the regression equation is \n",
    "associated with a random error, e. The random errors are normally distributed with mean 0 and standard deviation, \n",
    ". Finally, the random errors are independent of each other.\n",
    "\n",
    "3. Find the general formula of the multiple regression as shown below:\n",
    "y^ = b0+b1x1+b2x2+b3x3......bkxk\n",
    "\n",
    "where k denotes the number of predictor variables, b0,b1,b2....bk denote the coefficients of the independent \n",
    "variables x1,x2,x3.....xk\n",
    ".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b700a1ef-a247-479a-91bf-4d1e21fae825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear regression is an analysis that assesses whether one or more predictor variables explain the \\ndependent (criterion) variable.  The regression has five key assumptions:\\n\\nLinear relationship:\\n    linear regression needs the relationship between the independent and dependent variables to be linear.  \\n    It is also important to check for outliers since linear regression is sensitive to outlier effects.\\nMultivariate normality:\\n    the linear regression analysis requires all variables to be multivariate normal.  This assumption can best be \\n    checked with a histogram or a Q-Q-Plot.  Normality can be checked with a goodness of fit test, e.g., the \\n    Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation \\n    (e.g., log-transformation) might fix this issue.\\nNo or little multicollinearity:\\n    linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs \\n    when the independent variables are too highly correlated with each other.\\n    \\n    Multicollinearity may be tested with three central criteria:\\n    1) Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent \\n    variables the correlation coefficients need to be smaller than 1.\\n    \\n    2) Tolerance – the tolerance measures the influence of one independent variable on all other independent \\n    variables; the tolerance is calculated with an initial linear regression analysis. Tolerance is defined as \\n    T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the \\n    data and with T < 0.01 there certainly is.\\n    \\n    3) Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as \\n    VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is \\n    certainly multicollinearity among the variables.\\n    \\n    If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from \\n    each score) might help to solve the problem.  However, the simplest way to address the problem is to remove \\n    independent variables with high VIF values.\\nNo auto-correlation:\\n    linear regression analysis requires that there is little or no autocorrelation in the data. Autocorrelation \\n    occurs when the residuals are not independent from each other. For instance, this typically occurs in stock \\n    prices, where the price is not independent from the previous price.\\nHomoscedasticity:\\n    The scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal \\n    across the regression line). The scatter plots show data that are not homoscedastic (i.e., heteroscedastic)\\n    The Goldfeld-Quandt Test can also be used to test for heteroscedasticity. The test splits the data into two \\n    groups and tests to see if the variances of the residuals are similar across the groups. If homoscedasticity \\n    is present, a non-linear correction might fix the problem.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Linear regression is an analysis that assesses whether one or more predictor variables explain the \n",
    "dependent (criterion) variable.  The regression has five key assumptions:\n",
    "\n",
    "Linear relationship:\n",
    "    linear regression needs the relationship between the independent and dependent variables to be linear.  \n",
    "    It is also important to check for outliers since linear regression is sensitive to outlier effects.\n",
    "Multivariate normality:\n",
    "    the linear regression analysis requires all variables to be multivariate normal.  This assumption can best be \n",
    "    checked with a histogram or a Q-Q-Plot.  Normality can be checked with a goodness of fit test, e.g., the \n",
    "    Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation \n",
    "    (e.g., log-transformation) might fix this issue.\n",
    "No or little multicollinearity:\n",
    "    linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs \n",
    "    when the independent variables are too highly correlated with each other.\n",
    "    \n",
    "    Multicollinearity may be tested with three central criteria:\n",
    "    1) Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent \n",
    "    variables the correlation coefficients need to be smaller than 1.\n",
    "    \n",
    "    2) Tolerance – the tolerance measures the influence of one independent variable on all other independent \n",
    "    variables; the tolerance is calculated with an initial linear regression analysis. Tolerance is defined as \n",
    "    T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the \n",
    "    data and with T < 0.01 there certainly is.\n",
    "    \n",
    "    3) Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as \n",
    "    VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is \n",
    "    certainly multicollinearity among the variables.\n",
    "    \n",
    "    If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from \n",
    "    each score) might help to solve the problem.  However, the simplest way to address the problem is to remove \n",
    "    independent variables with high VIF values.\n",
    "No auto-correlation:\n",
    "    linear regression analysis requires that there is little or no autocorrelation in the data. Autocorrelation \n",
    "    occurs when the residuals are not independent from each other. For instance, this typically occurs in stock \n",
    "    prices, where the price is not independent from the previous price.\n",
    "Homoscedasticity:\n",
    "    The scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal \n",
    "    across the regression line). The scatter plots show data that are not homoscedastic (i.e., heteroscedastic)\n",
    "    The Goldfeld-Quandt Test can also be used to test for heteroscedasticity. The test splits the data into two \n",
    "    groups and tests to see if the variances of the residuals are similar across the groups. If homoscedasticity \n",
    "    is present, a non-linear correction might fix the problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41f0384-29e6-4cd4-bdc4-ef1135f5381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. \\nThe slope and the intercept define the linear relationship between two variables, and can be used to estimate an \\naverage rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of \\nchange.\\n\\nUsually, this relationship can be represented by the equation y = b0 + b1x, where b0 is the y-intercept and b1 is \\nthe slope.\\n\\nFor example, a company determines that job performance for employees in a production department can be predicted \\nusing the regression model y = 130 + 4.3x, where x is the hours of in-house training they receive (from 0 to 20) \\nand y is their score on a job skills test. The value of the y-intercept (130) indicates the average job skill \\nscore for an employee with no training. The value of the slope (4.3) indicates that for each hour of training, \\nthe job skill score increases, on average, by 4.3 points.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. \n",
    "The slope and the intercept define the linear relationship between two variables, and can be used to estimate an \n",
    "average rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of \n",
    "change.\n",
    "\n",
    "Usually, this relationship can be represented by the equation y = b0 + b1x, where b0 is the y-intercept and b1 is \n",
    "the slope.\n",
    "\n",
    "For example, a company determines that job performance for employees in a production department can be predicted \n",
    "using the regression model y = 130 + 4.3x, where x is the hours of in-house training they receive (from 0 to 20) \n",
    "and y is their score on a job skills test. The value of the y-intercept (130) indicates the average job skill \n",
    "score for an employee with no training. The value of the slope (4.3) indicates that for each hour of training, \n",
    "the job skill score increases, on average, by 4.3 points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0c4e07-e44f-4aad-89a2-2b7b9e605d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA gradient simply measures the change in all weights with regard to the change in error. We can also think of a \\ngradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can \\nlearn. But if the slope is zero, the model stops learning. In mathematical terms, a gradient is a partial \\nderivative with respect to its inputs.\\n\\nGradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient \\ndescent in machine learning is simply used to find the values of a function's parameters (coefficients) that \\nminimize a cost function as far as possible.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "A gradient simply measures the change in all weights with regard to the change in error. We can also think of a \n",
    "gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can \n",
    "learn. But if the slope is zero, the model stops learning. In mathematical terms, a gradient is a partial \n",
    "derivative with respect to its inputs.\n",
    "\n",
    "Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient \n",
    "descent in machine learning is simply used to find the values of a function's parameters (coefficients) that \n",
    "minimize a cost function as far as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a81663-6e7b-4700-a3b4-f07f4a52b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor complex connections between data, the relationship might be explained by more than one variable. In this case, \\nan analyst uses multiple regression which attempts to explain a dependent variable using more than one independent \\nvariable.\\n\\nThere are two main uses for multiple regression analysis. The first is to determine the dependent variable based \\non multiple independent variables. For example, we may be interested in determining what a crop yield will be \\nbased on temperature, rainfall, and other independent variables. The second is to determine how strong the \\nrelationship is between each variable. For example, we may be interested in knowing how a crop yield will change \\nif rainfall increases or the temperature decreases.\\n\\nSimple linear regression establishes the relationship between two variables. Simple Linear regression is \\ngraphically depicted using a straight line with the slope defining how the change in one variable impacts a change \\nin the other. \\nIn Simple linear regression, every dependent value has a single corresponding independent variable that drives its \\nvalue. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "For complex connections between data, the relationship might be explained by more than one variable. In this case, \n",
    "an analyst uses multiple regression which attempts to explain a dependent variable using more than one independent \n",
    "variable.\n",
    "\n",
    "There are two main uses for multiple regression analysis. The first is to determine the dependent variable based \n",
    "on multiple independent variables. For example, we may be interested in determining what a crop yield will be \n",
    "based on temperature, rainfall, and other independent variables. The second is to determine how strong the \n",
    "relationship is between each variable. For example, we may be interested in knowing how a crop yield will change \n",
    "if rainfall increases or the temperature decreases.\n",
    "\n",
    "Simple linear regression establishes the relationship between two variables. Simple Linear regression is \n",
    "graphically depicted using a straight line with the slope defining how the change in one variable impacts a change \n",
    "in the other. \n",
    "In Simple linear regression, every dependent value has a single corresponding independent variable that drives its \n",
    "value. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673f4db4-0c5e-42f0-a331-80c59fc70b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlinear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs \\n    when the independent variables are too highly correlated with each other.\\n    \\n    Multicollinearity may be tested with three central criteria:\\n    1) Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent \\n    variables the correlation coefficients need to be smaller than 1.\\n    \\n    2) Tolerance – the tolerance measures the influence of one independent variable on all other independent \\n    variables; the tolerance is calculated with an initial linear regression analysis. Tolerance is defined as \\n    T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the \\n    data and with T < 0.01 there certainly is.\\n    \\n    3) Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as \\n    VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is \\n    certainly multicollinearity among the variables.\\n    \\n    If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from \\n    each score) might help to solve the problem.  However, the simplest way to address the problem is to remove \\n    independent variables with high VIF values.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs \n",
    "    when the independent variables are too highly correlated with each other.\n",
    "    \n",
    "    Multicollinearity may be tested with three central criteria:\n",
    "    1) Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent \n",
    "    variables the correlation coefficients need to be smaller than 1.\n",
    "    \n",
    "    2) Tolerance – the tolerance measures the influence of one independent variable on all other independent \n",
    "    variables; the tolerance is calculated with an initial linear regression analysis. Tolerance is defined as \n",
    "    T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the \n",
    "    data and with T < 0.01 there certainly is.\n",
    "    \n",
    "    3) Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as \n",
    "    VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is \n",
    "    certainly multicollinearity among the variables.\n",
    "    \n",
    "    If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from \n",
    "    each score) might help to solve the problem.  However, the simplest way to address the problem is to remove \n",
    "    independent variables with high VIF values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3cdb23-332c-4047-8832-11d450a5aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPolynomial regression is a type of regression analysis that is used when the relationship between the dependent \\nand independent variables is not linear. It involves fitting a polynomial function to the data points to obtain a \\ncurve that represents the relationship between the variables.\\nThe equation for a polynomial regression model can be written as:\\nY = a + b1X + b2X^2 + ... + bnx^n\\n\\nLinear regression is a statistical technique used to find the linear relationship between a dependent variable and \\none or more independent variables. It is based on the assumption that there exists a linear relationship between \\nthe variables, and it uses a straight line to represent this relationship.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Polynomial regression is a type of regression analysis that is used when the relationship between the dependent \n",
    "and independent variables is not linear. It involves fitting a polynomial function to the data points to obtain a \n",
    "curve that represents the relationship between the variables.\n",
    "The equation for a polynomial regression model can be written as:\n",
    "Y = a + b1X + b2X^2 + ... + bnx^n\n",
    "\n",
    "Linear regression is a statistical technique used to find the linear relationship between a dependent variable and \n",
    "one or more independent variables. It is based on the assumption that there exists a linear relationship between \n",
    "the variables, and it uses a straight line to represent this relationship.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4d99bd-5936-43c9-863c-7e9e2cc6803f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPolynomial regression is a type of regression analysis that models the relationship between a response variable and \\none or more predictor variables by fitting a polynomial equation to the data. Compared to linear regression, which \\nmodels the relationship using a straight line, polynomial regression has several advantages:\\n\\nCapturing nonlinearity: Polynomial regression can capture nonlinearity in the relationship between the response \\n    variable and the predictor variable(s). This is important when the relationship is not well approximated by a \\n    straight line, and when there are curvatures or nonlinear patterns in the data.\\nImproved fit: Polynomial regression can often result in a better fit to the data than linear regression. This is \\n    because the polynomial function can capture more complex relationships between the variables, which can \\n    improve the accuracy of the model.\\nMore flexibility: Polynomial regression allows for more flexibility in modeling the relationship between the \\n    variables. This is because polynomial functions can be customized to fit the specific needs of the analysis, \\n    such as by including higher-order terms or interactions between the predictor variables.\\nBetter prediction: Polynomial regression can often provide better predictions than linear regression. This is \\n    because it can capture more of the variability in the data, and can make more accurate predictions for new \\n    data points that are not included in the original dataset.\\n\\nHowever, there are also some potential disadvantages to using polynomial regression, such as overfitting the data, \\nincreased complexity of the model, and difficulty in interpreting the results.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Polynomial regression is a type of regression analysis that models the relationship between a response variable and \n",
    "one or more predictor variables by fitting a polynomial equation to the data. Compared to linear regression, which \n",
    "models the relationship using a straight line, polynomial regression has several advantages:\n",
    "\n",
    "Capturing nonlinearity: Polynomial regression can capture nonlinearity in the relationship between the response \n",
    "    variable and the predictor variable(s). This is important when the relationship is not well approximated by a \n",
    "    straight line, and when there are curvatures or nonlinear patterns in the data.\n",
    "Improved fit: Polynomial regression can often result in a better fit to the data than linear regression. This is \n",
    "    because the polynomial function can capture more complex relationships between the variables, which can \n",
    "    improve the accuracy of the model.\n",
    "More flexibility: Polynomial regression allows for more flexibility in modeling the relationship between the \n",
    "    variables. This is because polynomial functions can be customized to fit the specific needs of the analysis, \n",
    "    such as by including higher-order terms or interactions between the predictor variables.\n",
    "Better prediction: Polynomial regression can often provide better predictions than linear regression. This is \n",
    "    because it can capture more of the variability in the data, and can make more accurate predictions for new \n",
    "    data points that are not included in the original dataset.\n",
    "\n",
    "However, there are also some potential disadvantages to using polynomial regression, such as overfitting the data, \n",
    "increased complexity of the model, and difficulty in interpreting the results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb94b9-8af7-43c0-b683-6bfb12a98346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
