{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5ce402-23d1-4b21-a4a2-0e0e12987d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRidge regression is a model tuning method that is used to analyse any data that suffers from \\nmulticollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, \\nleast-squares are unbiased, and variances are large, this results in predicted values being far away from \\nthe actual values. \\n\\nThe cost function for ridge regression:\\n\\nMin(||Y – X(theta)||^2 + λ||theta||^2)\\n\\nLambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by \\nchanging the values of alpha, we are controlling the penalty term. The higher the values of alpha, the \\nbigger is the penalty and therefore the magnitude of coefficients is reduced.\\n\\nIt shrinks the parameters. Therefore, it is used to prevent multicollinearity\\nIt reduces the model complexity by coefficient shrinkage\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from \n",
    "multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, \n",
    "least-squares are unbiased, and variances are large, this results in predicted values being far away from \n",
    "the actual values. \n",
    "\n",
    "The cost function for ridge regression:\n",
    "\n",
    "Min(||Y – X(theta)||^2 + λ||theta||^2)\n",
    "\n",
    "Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by \n",
    "changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the \n",
    "bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "\n",
    "It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
    "It reduces the model complexity by coefficient shrinkage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536e1fde-181e-4346-b3dc-f7612e0cbbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, \\nand independence. However, as ridge regression does not provide confidence limits, the distribution of \\nerror\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, \n",
    "and independence. However, as ridge regression does not provide confidence limits, the distribution of \n",
    "error\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab85ab3-92ff-49a9-bb17-7214dc699895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in \\nridge regression and lasso regression. It is basically the amount of shrinkage, where data values are \\nshrunk towards a central point, like the mean.\\n\\nAlthough there isn’t an “optimal” tuning parameter for any particular scenario, finding one is necessary \\nfor any analysis involving high-dimensional data. Fan & Tang recommend:\\n\\n1. Choose a regularization method. For example:\\nLasso regression (L1)\\nRidge Regression (L2)\\n2. Smoothly clipped absolute deviation (SCAD)\\n3. Elastic net (a combination of L1 and L2)\\n4. Adaptive lasso\\n5. Use a sequence of tuning parameters to create a series of different models.\\n6. Study the different models and select one that best fits our needs. Various methods for model selection \\nexist, including: Mallow’s Cp, Akaike’s Information Criterion (AIC) and Bayesian Information Criterion (BIC).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in \n",
    "ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are \n",
    "shrunk towards a central point, like the mean.\n",
    "\n",
    "Although there isn’t an “optimal” tuning parameter for any particular scenario, finding one is necessary \n",
    "for any analysis involving high-dimensional data. Fan & Tang recommend:\n",
    "\n",
    "1. Choose a regularization method. For example:\n",
    "Lasso regression (L1)\n",
    "Ridge Regression (L2)\n",
    "2. Smoothly clipped absolute deviation (SCAD)\n",
    "3. Elastic net (a combination of L1 and L2)\n",
    "4. Adaptive lasso\n",
    "5. Use a sequence of tuning parameters to create a series of different models.\n",
    "6. Study the different models and select one that best fits our needs. Various methods for model selection \n",
    "exist, including: Mallow’s Cp, Akaike’s Information Criterion (AIC) and Bayesian Information Criterion (BIC).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071219eb-4979-4a06-9a52-2c435a03334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRidge regression and Lasso regression are two popular techniques that make use of regularization for \\npredicting.\\nBoth the techniques work by penalizing the magnitude of coefficients of features along with minimizing the \\nerror between predictions and actual values or records.\\n\\nThe key difference however, between Ridge and Lasso regression is that Lasso Regression has the ability to \\nnullify the impact of an irrelevant feature in the data, meaning that it can reduce the coefficient of a \\nfeature to zero thus completely eliminating it and hence is better at reducing the variance when the data \\nconsists of many insignificant features. Ridge regression, however, can not reduce the coefficients to \\nabsolute zero.\\n\\nRidge regression performs better when the data consists of features which are sure to be more relevant and \\nuseful.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Ridge regression and Lasso regression are two popular techniques that make use of regularization for \n",
    "predicting.\n",
    "Both the techniques work by penalizing the magnitude of coefficients of features along with minimizing the \n",
    "error between predictions and actual values or records.\n",
    "\n",
    "The key difference however, between Ridge and Lasso regression is that Lasso Regression has the ability to \n",
    "nullify the impact of an irrelevant feature in the data, meaning that it can reduce the coefficient of a \n",
    "feature to zero thus completely eliminating it and hence is better at reducing the variance when the data \n",
    "consists of many insignificant features. Ridge regression, however, can not reduce the coefficients to \n",
    "absolute zero.\n",
    "\n",
    "Ridge regression performs better when the data consists of features which are sure to be more relevant and \n",
    "useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9959084-96cf-4004-9d76-36c47ef331fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulticollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression\\naims at reducing the standard error by adding some bias in the estimates of the regression. The reduction \\nof the standard error in regression estimates significantly increases the reliability of the estimates.\\n\\nThe detection of multicollinearity is key to the reduction of standard errors in models for predictability \\nefficiency. First, one can detect by investigating independent variables for correlation in pairwise \\nscatter plots. High pairwise correlations of independent variables can mean the presence of \\nmulticollinearity.\\n\\nSecondly, one can detect multicollinearity through the consideration of Variance Inflation Factors (VIFs). \\nA VIF score of 10 or more shows that variables are collinear. Thirdly, one can detect multicollinearity by \\nchecking whether the correlation matrix eigenvalues are close to zero. One should use the condition numbers,\\nas opposed to using the eigenvalue numerical sizes.  The larger the condition numbers, the more the \\nmulticollinearity.\\n\\nMulticollinearity correction depends on the cause. When the source of collinearity is data collection, \\nfor example, the correction will involve collecting additional data from the proper subpopulation. If the \\ncause is the linear model choice, the correction will include simplifying the model by the proper variable \\nselection methods. If the causes of multicollinearity are certain observations, eliminate the observations. \\nRidge regression is also an effective eliminator of multicollinearity.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression\n",
    "aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction \n",
    "of the standard error in regression estimates significantly increases the reliability of the estimates.\n",
    "\n",
    "The detection of multicollinearity is key to the reduction of standard errors in models for predictability \n",
    "efficiency. First, one can detect by investigating independent variables for correlation in pairwise \n",
    "scatter plots. High pairwise correlations of independent variables can mean the presence of \n",
    "multicollinearity.\n",
    "\n",
    "Secondly, one can detect multicollinearity through the consideration of Variance Inflation Factors (VIFs). \n",
    "A VIF score of 10 or more shows that variables are collinear. Thirdly, one can detect multicollinearity by \n",
    "checking whether the correlation matrix eigenvalues are close to zero. One should use the condition numbers,\n",
    "as opposed to using the eigenvalue numerical sizes.  The larger the condition numbers, the more the \n",
    "multicollinearity.\n",
    "\n",
    "Multicollinearity correction depends on the cause. When the source of collinearity is data collection, \n",
    "for example, the correction will involve collecting additional data from the proper subpopulation. If the \n",
    "cause is the linear model choice, the correction will include simplifying the model by the proper variable \n",
    "selection methods. If the causes of multicollinearity are certain observations, eliminate the observations. \n",
    "Ridge regression is also an effective eliminator of multicollinearity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffd2a9b-8e43-4159-80a1-b27e6a2efc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRidge regression is used for regression purpose only as it needs the dependent variable to be continuous.\\n\\nSo for the analysis Ridge regression will be used which works fine in presence of multicollinearity but \\nwith a continuous dependent variable.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous.\n",
    "\n",
    "So for the analysis Ridge regression will be used which works fine in presence of multicollinearity but \n",
    "with a continuous dependent variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed7574e-08ef-4941-9671-4dff8c2880a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe ridge regression will penalize your coefficients, such that those that are the least effective in your \\nestimation will \"shrink\" the fastest.\\n\\nImagine you have a budget allocated and each coefficient can take some to play a role in the estimation. \\nNaturally those who are more important will take more of the budget. As you increase the lambda, you are \\ndecreasing the budget, i.e. penalizing more.\\n\\nFor your plot, each line represents a coefficient whose value is going to zero as you are decreasing the \\nbudget or as you are penalizing more(increasing the lambda). To choose the best lambda, you should consult \\nthe MSE vs lambda plot. \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The ridge regression will penalize your coefficients, such that those that are the least effective in your \n",
    "estimation will \"shrink\" the fastest.\n",
    "\n",
    "Imagine you have a budget allocated and each coefficient can take some to play a role in the estimation. \n",
    "Naturally those who are more important will take more of the budget. As you increase the lambda, you are \n",
    "decreasing the budget, i.e. penalizing more.\n",
    "\n",
    "For your plot, each line represents a coefficient whose value is going to zero as you are decreasing the \n",
    "budget or as you are penalizing more(increasing the lambda). To choose the best lambda, you should consult \n",
    "the MSE vs lambda plot. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dacbdf0b-3894-49f9-b6b6-a49cea7a7c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRidge regression is an expansion of linear regression. It’s fundamentally a regularization of the linear \\nregression model. Ridge regression uses the damping factor (λ) as a scalar that should be learned, \\nnormally it will utilize a method called cross-validation to find the value. We will calculate the damping \\nfactor/ridge regression in the ridge regression (RR) model firsthand to minimize the running time used \\nwhen using cross-validation. The RR model will be used to forecast the time-series data. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Ridge regression is an expansion of linear regression. It’s fundamentally a regularization of the linear \n",
    "regression model. Ridge regression uses the damping factor (λ) as a scalar that should be learned, \n",
    "normally it will utilize a method called cross-validation to find the value. We will calculate the damping \n",
    "factor/ridge regression in the ridge regression (RR) model firsthand to minimize the running time used \n",
    "when using cross-validation. The RR model will be used to forecast the time-series data. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544f4db-756d-42d1-90d7-4c1045cb8978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
