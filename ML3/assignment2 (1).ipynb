{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a034221f-188b-457a-a9b7-760215540b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nR-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the \\npercentage of the variance in the dependent variable that the independent variables explain collectively. \\nR-squared measures the strength of the relationship between model and the dependent variable on a \\nconvenient 0 – 100% scale.\\n\\nR-squared evaluates the scatter of the data points around the fitted regression line. It is also called \\nthe coefficient of determination, or the coefficient of multiple determination for multiple regression. \\nFor the same data set, higher R-squared values represent smaller differences between the observed data and\\nthe fitted values.\\n\\nR-squared is the percentage of the dependent variable variation that a linear model explains.\\n\\nR-squared = variance explained by the model / total variance.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the \n",
    "percentage of the variance in the dependent variable that the independent variables explain collectively. \n",
    "R-squared measures the strength of the relationship between model and the dependent variable on a \n",
    "convenient 0 – 100% scale.\n",
    "\n",
    "R-squared evaluates the scatter of the data points around the fitted regression line. It is also called \n",
    "the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n",
    "For the same data set, higher R-squared values represent smaller differences between the observed data and\n",
    "the fitted values.\n",
    "\n",
    "R-squared is the percentage of the dependent variable variation that a linear model explains.\n",
    "\n",
    "R-squared = variance explained by the model / total variance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c1f520-d8a8-413c-a0a3-7400d29599d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdjusted r squared is similar to r-squared and measures the variation in the target variable. Still, \\nunlike r-squared, it takes only those independent variables with some significance and penalizes for \\nadding features that are not significant for predicting the dependent variable.\\n\\nR-squared increases every time we add an independent variable to the model. The R-squared never decreases. \\nA regression model that contains more independent variables than another model can look like it provides \\na better fit merely because it contains more variables. Whereas, in adjusted R-squared value actually \\ndecreases when the variable doesn’t improve the model fit by a sufficient amount.\\n\\nWhen a model contains an excessive number of independent variables and polynomial terms, it becomes \\noverly customized to fit the peculiarities and random noise in sample rather than reflecting the entire \\npopulation. Whereas, in adjusted R-squared, overfitting issue never happened as value actually \\ndecreases when the variable doesn’t improve the model fit by a sufficient amount\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Adjusted r squared is similar to r-squared and measures the variation in the target variable. Still, \n",
    "unlike r-squared, it takes only those independent variables with some significance and penalizes for \n",
    "adding features that are not significant for predicting the dependent variable.\n",
    "\n",
    "R-squared increases every time we add an independent variable to the model. The R-squared never decreases. \n",
    "A regression model that contains more independent variables than another model can look like it provides \n",
    "a better fit merely because it contains more variables. Whereas, in adjusted R-squared value actually \n",
    "decreases when the variable doesn’t improve the model fit by a sufficient amount.\n",
    "\n",
    "When a model contains an excessive number of independent variables and polynomial terms, it becomes \n",
    "overly customized to fit the peculiarities and random noise in sample rather than reflecting the entire \n",
    "population. Whereas, in adjusted R-squared, overfitting issue never happened as value actually \n",
    "decreases when the variable doesn’t improve the model fit by a sufficient amount\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd5aa4e-5418-4ba0-8bfa-b544720c9288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the case of multivariate linear regression, if we keep adding different variables, the value of the \\nr-square will either remain the same or increase, irrespective of the significance of the variable.\\nHere, adjusted R-Squared comes into the picture. Adjusted r-squared calculates the R squared from only \\nthose variables whose addition in the model is significant. It also penalizes for adding variables that \\ndo not improve the existing model.\\n\\n- It is advised to use adjusted r squared while using linear regression for multivariable.\\n- Adding a non-significant variable in the model increases the difference between r-squared and adjusted \\nr-squared.\\n- The value of the r-squared can’t be less than zero, but the value of the adjusted r-squared can be \\nnegative.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "In the case of multivariate linear regression, if we keep adding different variables, the value of the \n",
    "r-square will either remain the same or increase, irrespective of the significance of the variable.\n",
    "Here, adjusted R-Squared comes into the picture. Adjusted r-squared calculates the R squared from only \n",
    "those variables whose addition in the model is significant. It also penalizes for adding variables that \n",
    "do not improve the existing model.\n",
    "\n",
    "- It is advised to use adjusted r squared while using linear regression for multivariable.\n",
    "- Adding a non-significant variable in the model increases the difference between r-squared and adjusted \n",
    "r-squared.\n",
    "- The value of the r-squared can’t be less than zero, but the value of the adjusted r-squared can be \n",
    "negative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9664d649-b89d-42cb-94ec-ed7845cab925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe objective of Linear Regression is to find a line that minimizes the prediction error of all the data \\npoints. The Mean Squared Error, Mean absolute error, Root Mean Squared Error, and R-Squared or Coefficient\\nof determination metrics are used to evaluate the performance of the model in regression analysis.\\n\\n1. The Mean absolute error represents the average of the absolute difference between the actual and \\npredicted values in the dataset. It measures the average of the residuals in the dataset.\\n\\n2. Mean Squared Error represents the average of the squared difference between the original and predicted \\nvalues in the data set. It measures the variance of the residuals.\\n\\n3. Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation \\nof residuals.\\n\\n4. The coefficient of determination or R-squared represents the proportion of the variance in the \\ndependent variable which is explained by the linear regression model. It is a scale-free score i.e. \\nirrespective of the values being small or large, the value of R square will be less than one.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The objective of Linear Regression is to find a line that minimizes the prediction error of all the data \n",
    "points. The Mean Squared Error, Mean absolute error, Root Mean Squared Error, and R-Squared or Coefficient\n",
    "of determination metrics are used to evaluate the performance of the model in regression analysis.\n",
    "\n",
    "1. The Mean absolute error represents the average of the absolute difference between the actual and \n",
    "predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "2. Mean Squared Error represents the average of the squared difference between the original and predicted \n",
    "values in the data set. It measures the variance of the residuals.\n",
    "\n",
    "3. Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation \n",
    "of residuals.\n",
    "\n",
    "4. The coefficient of determination or R-squared represents the proportion of the variance in the \n",
    "dependent variable which is explained by the linear regression model. It is a scale-free score i.e. \n",
    "irrespective of the values being small or large, the value of R square will be less than one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8776c72-6f53-469d-be77-44b32f2f6723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMean absolute Error (MAE): \\n1. It doesn’t account for the direction of the value. Even if value is negative, positive value is used \\nfor calculation.\\n2. MAE is less biased for higher values. It may not adequately reflect the performance when dealing with \\nlarge error values\\n3. MAE doesn’t necessarily penalize large errors.\\n4. MAE is more useful when the overall impact is proportionate to the actual increase in error. \\nFor example- if error values go up to 6 from 3, actual impact on the result is twice. It is more common \\nin financial industry where a loss of 6 would be twice of 3.\\n\\nMean square Error (MSE):\\n1. It does account for positive or negative value.\\n2. MSE is highly biased for higher values.\\n3. MSE penalize large errors\\n\\nRoot mean square error (RMSE):\\n1. It does account for positive or negative value.\\n2. RMSE is better in terms of reflecting performance when dealing with large error values\\n3. RMSE penalize large errors.\\n4. RMSE is more useful when the overall impact is disproportionate to the actual increase in error. \\nFor example- if error values go up to 6 from 3, actual impact on the result is more than twice. This \\ncould be common in clinical trials, as error goes up, overall impact goes up disproportionately\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Mean absolute Error (MAE): \n",
    "1. It doesn’t account for the direction of the value. Even if value is negative, positive value is used \n",
    "for calculation.\n",
    "2. MAE is less biased for higher values. It may not adequately reflect the performance when dealing with \n",
    "large error values\n",
    "3. MAE doesn’t necessarily penalize large errors.\n",
    "4. MAE is more useful when the overall impact is proportionate to the actual increase in error. \n",
    "For example- if error values go up to 6 from 3, actual impact on the result is twice. It is more common \n",
    "in financial industry where a loss of 6 would be twice of 3.\n",
    "\n",
    "Mean square Error (MSE):\n",
    "1. It does account for positive or negative value.\n",
    "2. MSE is highly biased for higher values.\n",
    "3. MSE penalize large errors\n",
    "\n",
    "Root mean square error (RMSE):\n",
    "1. It does account for positive or negative value.\n",
    "2. RMSE is better in terms of reflecting performance when dealing with large error values\n",
    "3. RMSE penalize large errors.\n",
    "4. RMSE is more useful when the overall impact is disproportionate to the actual increase in error. \n",
    "For example- if error values go up to 6 from 3, actual impact on the result is more than twice. This \n",
    "could be common in clinical trials, as error goes up, overall impact goes up disproportionately\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8804babb-e5c5-4465-be44-38da4c911b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in \\nlinear regression models by including a penalty term in the cost function. In contrast to Ridge regression, \\nit adds the total of the absolute values of the coefficients rather than the sum of the squared \\ncoefficients.\\n1. Encourages some coefficients to be exactly zero\\n2. Adds a penalty term proportional to the sum of absolute values of coefficients\\n3. Can eliminate some features\\n4. Suitable when some features are irrelevant or redundant\\n\\nWhereas ridge regression is a regularization approach. The size of the coefficients is reduced and \\noverfitting is prevented by adding a penalty term to the cost function of linear regression. \\nThe penalty term regulates the magnitude of the coefficients in the model and is proportional to the \\nsum of squared coefficients.\\n1.Shrinks the coefficients toward zero\\n2. Adds a penalty term proportional to the sum of squared coefficients\\n3. Does not eliminate any features\\n4. Suitable when all features are importantly\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in \n",
    "linear regression models by including a penalty term in the cost function. In contrast to Ridge regression, \n",
    "it adds the total of the absolute values of the coefficients rather than the sum of the squared \n",
    "coefficients.\n",
    "1. Encourages some coefficients to be exactly zero\n",
    "2. Adds a penalty term proportional to the sum of absolute values of coefficients\n",
    "3. Can eliminate some features\n",
    "4. Suitable when some features are irrelevant or redundant\n",
    "\n",
    "Whereas ridge regression is a regularization approach. The size of the coefficients is reduced and \n",
    "overfitting is prevented by adding a penalty term to the cost function of linear regression. \n",
    "The penalty term regulates the magnitude of the coefficients in the model and is proportional to the \n",
    "sum of squared coefficients.\n",
    "1.Shrinks the coefficients toward zero\n",
    "2. Adds a penalty term proportional to the sum of squared coefficients\n",
    "3. Does not eliminate any features\n",
    "4. Suitable when all features are importantly\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3126669e-6c4a-480e-9674-dc735b227265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel overfitting is a serious problem and can cause the model to produce misleading information. One of \\nthe techniques to overcome overfitting is Regularization. Regularization, in general, penalizes the \\ncoefficients that cause the overfitting of the model.\\nTwo well-liked regularization methods for linear regression models are ridge and lasso regression. They \\nhelp to solve the overfitting issue, which arises when a model is overly complicated and fits the training \\ndata too well, leading to worse performance on fresh data. \\nRidge regression reduces the size of the coefficients and prevents overfitting by introducing a penalty \\nelement to the cost function of linear regression. The squared coefficient total is directly proportional \\nto this penalty component.  \\nLasso regression, a penalty term is added that is proportionate to the total of the absolute values of the \\ncoefficients. This promotes some of the coefficients to approach 0 exactly, rendering some aspects of the \\nmodel utterly irrelevant.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Model overfitting is a serious problem and can cause the model to produce misleading information. One of \n",
    "the techniques to overcome overfitting is Regularization. Regularization, in general, penalizes the \n",
    "coefficients that cause the overfitting of the model.\n",
    "Two well-liked regularization methods for linear regression models are ridge and lasso regression. They \n",
    "help to solve the overfitting issue, which arises when a model is overly complicated and fits the training \n",
    "data too well, leading to worse performance on fresh data. \n",
    "Ridge regression reduces the size of the coefficients and prevents overfitting by introducing a penalty \n",
    "element to the cost function of linear regression. The squared coefficient total is directly proportional \n",
    "to this penalty component.  \n",
    "Lasso regression, a penalty term is added that is proportionate to the total of the absolute values of the \n",
    "coefficients. This promotes some of the coefficients to approach 0 exactly, rendering some aspects of the \n",
    "model utterly irrelevant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d097b9-15aa-4cee-b63e-d7487e248504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThey include all the predictors in the final model.\\nThey are unable to perform feature selection.\\nThey shrink the coefficients towards zero.\\nThey trade the variance for bias.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "They include all the predictors in the final model.\n",
    "They are unable to perform feature selection.\n",
    "They shrink the coefficients towards zero.\n",
    "They trade the variance for bias.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fda5aa-7baf-463f-9198-dd6cf9151ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel B is good as MAE value less then RMSE in model A\\n\\nRMSE (Root Mean Squared Error) is a popular metric for regression because it combines the advantages of \\nMSE (Mean Squared Error) while having the same scale as the target variable. It penalizes large errors \\nmore severely, making it sensitive to outliers.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Model B is good as MAE value less then RMSE in model A\n",
    "\n",
    "RMSE (Root Mean Squared Error) is a popular metric for regression because it combines the advantages of \n",
    "MSE (Mean Squared Error) while having the same scale as the target variable. It penalizes large errors \n",
    "more severely, making it sensitive to outliers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "579619e0-9f3b-4bb7-89d1-9256b4e9633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ10. You are comparing the performance of two regularized linear models using different types of\\nregularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\\nuses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\\nbetter performer, and why? Are there any trade-offs or limitations to your choice of regularization\\nmethod?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9105225-9441-44ac-80c3-509c34ea38ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
